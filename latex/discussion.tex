\chapter{Discussion}
\label{ch:discuss}
 
In this thesis we proposed two methods of variable selection for Bayesian decision trees, and we demonstrated their necessity in comparison to currently used methods such as  the approaches of Chipman et al. \cite{chipman1998bayesian} and Denison et al. \cite{denison1998bayesian}. Moreover, we showed the drawbacks of simplistic methods of variable selection such as the pruning rule and bootstrapping. While we focused on the variable selection aspects of the study of decision trees, there are several avenues forward from here and this chapter discusses those paths. 

The ability to provide good fitting decision trees with accurate predictions is of vital interest to practitioners in many fields. The use of simplistic models in the terminal nodes, such as a constant mean model or a highest class probability model, will inevitably lead to model misspecification in some datasets, such as data with many zeros. The solution to this type of model misspecification problem is to use a zero-inflated model in each of the terminal nodes. The CGM model requires  that the terminal node parameters be integrated over, as shown in Equation \ref{eqn:tbup}, to facilitate Metropolis-Hastings samples. For the zero-inflated model this can be accomplished at least to the point of a finite series, which can be calculated exactly if the number of zero observations is not too large. If the number of zero observations is too large, a numerical approximation would suffice. In future work, we intend to apply this model to several simulated and real datasets. These will include the solder data analyzed by Lee \cite{lee2006decision}, and the nematode and vine root data collected by Giese et al. \cite{giese2014,giese2014complete}. Additionally, some of these datasets could benefit from the application of the ALoVaS method to maintain a shallow tree and to select variables, while using an appropriate terminal node model, such as a zero-inflated model.  

A second area of further research is studying the equivalence of the SSVS, lasso, horseshoe, and perhaps even the generalized double Pareto model proposed by Bhattacharya et al. \cite{bhattacharya2012bayesian}. By writing the negative log posteriors in proportional form, we find that the horseshoe prior and the lasso prior have similar forms when expanding the final term involving the logarithm. The appropriate Taylor expansion here is 

\begin{equation}\label{eqn:taylor_log_one_plus_x}
log\left(1+(\lambda/\sigma)^2/p\right) = (\lambda/\sigma)^2/p + (\lambda/\sigma)^4/2p^2 + \dots+ (\lambda/\sigma)^{2k}/kp + \dots.
\end{equation}
By expanding the negative log posterior in this way, we see the asymptotic equivalence of the parameter expanded lasso and the horseshoe posteriors. It is reasonable to assume that a similar approximation will hold for the generalized double Pareto prior whose density is given in Bhattacharya et al. \cite{bhattacharya2012bayesian}. While Carvalho et al. \cite{carvalho2010horseshoe} emphasize the importance of both the singularity, or at least positive point mass probability at zero, as well as the heavy tail behavior, it is clear that the difference in the two posteriors will be governed by a power exponential density where the variance is proportional to $1/p$. Thus, in high-dimensional models, the difference will be negligible and the sparsity patterns will be similar. This seems somewhat at odds with the claims of Carvalho et al. and the claims of Bhattacharya et al. and this is an area in need of further clarification. Moreover, it remains to be seen if there is a similar asymptotic relationship with the stochastic search method. We posit the relationship exists but the difficulty lies in specifying the appropriate limiting form of the prior to give us the SSVS model while simultaneously encapsulating the lasso and horseshoe priors. 



 