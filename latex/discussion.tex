\section{Discussion}
\label{ch:discuss}
 
In this thesis we demonstrated two methods of variable selection for Bayesian decision trees and the necessity of these methods in comparison to currently used methods like the approaches of Chipman et al. \cite{chipman1998bayesian} and Denison et al. \cite{denison1998bayesian}. Moreover, we showed the drawbacks of simplistic methods of variable selection like the pruning rule and bootstrapping. While we focused in this thesis on the variable selection aspects of the study of decision trees there are several avenues forward from here and this chapter discusses those paths. 

The ability to provide good fitting decision trees with accurate predictions is of vital interest to practitioners in several fields. The use of simplistic models in the terminal nodes like a constant mean model or a highest class probability model will inevitably lead to model misspecification in some datasets like data with a large occurrence of zeros. The solution to this type of model misspecification problem is to use a zero-inflated model in each of the terminal nodes. One of the requirements for the Metropolis-Hastings algorithm proposed by CGM is that the parameters can be integrated over as shown in Equation \ref{eqn:tbup}. For the zero-inflated model this can be accomplished at least to the point of a finite series which can be calculated exactly if the number of zero observations are not too large. If the number of zero observations is too large a numerical approximation would suffice. Future work will apply this model to several simulated and real datasets. These will include the solder data analyzed by \cite{lee2006decision}, the Nematode and vine root data collected by Giese et al. \cite{}. Moreover some of these datasets could find use in applying the ALoVaS method to maintain a shallow tree and to select variables while using an appropriate terminal node model like a zero-inflated model.  

A second area of further research is into the equivalence of the SSVS, lasso, horseshoe, and perhaps even the generalized double Pareto model proposed by Bhattacharya et al. \cite{bhattacharya2012bayesian}. By writing the negative log posteriors in proportional form we find that the horseshoe prior and the lasso prior have similar forms when expanding the final term involving the logarithm. The appropriate Taylor expansion here is 

\begin{equation}\label{eqn:taylor_log_one_plus_x}
log\left(1+(\lambda/\sigma)^2/p\right) = (\lambda/\sigma)^2/p + (\lambda/\sigma)^4/2p^2 + \dots+ (\lambda/\sigma)^{2k}/kp + \dots
\end{equation}

By expanding the negative log posterior in this way we are able to see the equivalence asymptotically of the parameter expanded lasso and the horses posteriors. It is reasonable to assume a similar approximation will hold for the generalized double Pareto prior  whose density is given in Bhattacharya et al. \cite{bhattacharya2012bayesian}. While Carvalho et al. \cite{carvalho2010horseshoe} emphasize the importance of both the singularity, or at least positive point mass probability at zero, as well as the heavy tail behavior it is clear the difference in the two posteriors will be governed by a power exponential density where the variance is proportional to $1/p$. Thus in high-dimensional models the difference will be negligible and the sparsity patterns will be similar. This seems somewhat at odds with the claims of Carvalho et al. and the claims of Bhattacharya et al. and is an area in need of further clarification. Moreover, it remains to be seen if there is a similar asymptotic relationship with the stochastic search method. We posit this to be the case but the difficulty lies in specifying the appropriate limiting form of the prior to give us the SSVS model while simultaneously encapsulating the lasso and horseshoe priors. 



 