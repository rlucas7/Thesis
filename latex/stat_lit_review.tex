%% stat lit review for decision trees 

%outline for this section: 
%P0: introduction
%P1: morgan and sonquist 1963
%P2:Kass 1980 paper 
%P3: Breiman et al 1984
%P4: Wei-Yin Loh & Vanichsetakul 1987 
%P5: Bayes -> CGM(1998) + DMS 
%P6: 1999 + 2004 papers by Shih on split selection rules
%P6: further work of CGM(2001, 2003)
%P7: Loh's guide algorithm 2008
%P8: Kjell doksum, Nonparametric variable selection: the EARTH algorithm (2008 JASA)
%P6: Gaussian Processes GL (2008)
%P7: Dynamic Trees (2012) TG, and the tgp R package
%P :Loh 2012 variable selection in decision trees lecture notes in stats article
%P8: conclusion  


%outline for this section:
%P0: intro 
%P1: BDD 1959 paper
%P2: quinlan (1980's) work maybe 2 paragraphs b/c a few papers and a book.
%P3: Secret (Alin Dobra ~2001)
%P4: Ant colony optimization approaches 2001-2003 (2 papers?)
%P5: ACO approaches 2007-2009 (2 papers)
%P6: Target,-> genetic algorithms 
%P7: Scaling machine learning/ decision trees for websearch
%P8:Conclusion
%

This section details two important aspects of this thesis: decision trees and variable selection. Little work has focused on variable selection in decision trees and, for this reason, we separate the literature review into two components. In subsequent chapters, we provide more details on the few implementations of variable selection in decision tree problems. We begin with an historical review of the decision tree literature. Then, in a subsequent subsection, we survey the literature on the variable selection problem. 

\subsubsection{An Historical Path Through the Literature}

In this section we review decision tree literature from an historical perspective. We begin with one of the earliest decision tree papers in the statistics literature, the paper by Morgan and Sonquist \cite{morgan1963problems}. Their proposal suggested that the linear model is often an inadequate method to handle complex survey data analysis. The authors outline several complications with survey data, including high correlations among the covariates, known to cause instability in linear models, and complex interactions, which make the linear model difficult to interpret and complicated to calculate. Furthermore, their paper cites only one previous author who had a similar but not the same approach. They mention an applied statistics paper from 1959 by Belson \cite{belson1959matching}, also an economist. Belson's work certainly predates Morgan and Sonquist's, but the aim of both papers appears to be to partition recursively complex survey data, applying an empirical and simple approach instead of a theoretical approach to analyzing large complicated data. 

After the pioneering work of Morgan and Sonquist and that of Belson, the next researcher to begin looking at decision tree methods of data analysis was Kass \cite{kass1975significance}. Although Kass' paper does not present a graphic of a decision tree, the method he proposes is that of recursive partitioning of the data. He gives suggestions and recommendations for how to carry out this procedure and notes the need for further research in this area. Kass follows this paper up with another paper in 1980 \cite{kass1980exploratory}. Kass \cite{kass1980exploratory} studies recursive partitioning again but this time specifically on categorical data. Conclusions and results are similar to those in Kass \cite{kass1975significance}. It is worth noting that although Breiman, Friedman, Olshen and Stone's 1984 CART \newabbrev{abbrev:CART}book \cite{breiman1984classification} is often considered the work that introduced decision trees into the statistics literature, here we have noted at least four papers prior to the 1984 publication of this book and we by no means claim to be exhaustive in our review.  Moreover, it is equally surprising that Breiman noted that they published the CART book as a book because the authors thought that no statistics journal would publish the work \cite{friedman2011remembering}.  

The work of Breiman et al. \cite{breiman1984classification} was pioneering in several aspects. The last chapter contains a theoretical proof of the consistency of decision trees as the number of observations, typically denoted $n$, grows arbitrarily large. Breiman et al. provided a new framework, which involved growing a full tree and then pruning back to optimality. This is the first instance of a consistent stopping rule in the decision tree literature. In addition, the book presents specifics of algorithmic implementation. Also many practical issues such as stopping criteria are thoroughly discussed, indicating why the full growth and then pruning approach is to be preferred over simpler stopping rules previously proposed. These aspects and more make the book excellent reading for theoretical statisticians and applied data analysts. Moreover, the low cost of the book and the practical interpretability of decision trees made the method popular among researchers in several fields.

Other work around the same time included the pioneering work of Quinlan \cite{quinlan1986induction} and his textbook \cite{quinlan1993c4}. Both Quinlan \cite{quinlan1986induction} and Quinlan \cite{quinlan1993c4} discuss induction learning for decision trees. Quinlan's research differs from the statistical approaches in several aspects, the most fundamental differences are Quinlan uses multiway splits compared to only binary decision tree splits in the previous statistics literature and Quinlan uses an information theoretic approach to justify decision trees whereas the statistics literature takes a nonparametric approach.  

Attempting to improve the prediction errors of CART trees, Loh and Vanichsetakul proposed using linear combinations of covariates as splitting rules instead of the simple axis orthogonal splits of the CART methodology. The fundamental differences between the CART method and the work of Loh and Vanichsetakul are: 1) multiple splits are possible at each node; 2) a direct stopping rule; 3) estimating missing values; 4) splits are linear combinations and may contain both categorical and continuous covariates;  5) no invariance to monotonic transformations; 6) Computationally faster than Breiman et al.'s method \cite{loh1988tree}. Furthermore, Loh and Vanichsetakul used statistical inference approaches such as $F$ ratios to choose splits and to stop the tree from growing. This work was criticized by Breiman and Friedman  \cite{breiman1988comment} on several aspects, the most important of which are the lack of robustness, no proof of a consistent stopping rule, and lack of invariance to monotonic transformations. 

In the subsequent decade much research was done, both empirically and theoretically regarding the efficacy of decision tree methods. The next major extension was done in the year 1998. Two groundbreaking papers were published, one by Chipman, George and McCulloch \cite{chipman1998bayesian}, and another by Denison, Mallick and Smith \cite{denison1998bayesian}, hereafter referred to as CGM \newabbrev{abbrev:CGM}and DMS \newabbrev{abbrev:DMS} respectively. Both articles brought Bayesian computational techniques to bear on the problem of decision tree induction. Much Bayesian computational work was accomplished following the groundbreaking Gibbs sampler first published in 1990 by Gelfand and Smith \cite{gelfand1990sampling}, such as Metropolis-Hastings (MH)\newabbrev{abbrev:MH} samplers \cite{hastings1970monte,robert1999monte} and reversible jump methods \cite{green1995reversible}. The papers by CGM and DMS brought the 8+ years of research in Markov chain Monte Carlo (MCMC \newabbrev{abbrev:MCMC}\hspace{-.1cm}) methods into decision tree search methods. CGM proposed a novel process prior and gave a set of proposal functions that exhibit useful cancellations in the MH ratio. In a similar vein, DMS proposed a reversible jump algorithm with a similar proposal function that appears to mix more efficiently while searching the space of trees. These two papers are the genesis of our developments in the current thesis. 

Around the same time (the 1990's), the machine learning community was experiencing rapid growth. During this time of rapid growth, Leo Breiman helped to bridge the gap between the statistics community and the machine learning community, publishing fundamental work proposing the bagging method \cite{breiman1996bagging}. During the same year, 1998, Ho proposed a similar generalization known as the random subspace method \cite{ho1998random}. Both algorithms use resampling methods similar to the bootstrap \cite{efron1997improvements,efron1994introduction}, but build a decision tree on each subsampled dataset and then aggregate the predictions from the resulting trees to improve prediction and stability of the estimator. Although more refined and developed, a similar approach is the random forest model \cite{breiman2001random}. The random forest model is the subject of current research into the theoretical properties of the resulting collection of trees and their predictions \cite{biau2008consistency,biau2012analysis}. Interestingly, this research has led to the conclusion that the pruning rule makes a consistent decision tree but that an analyst may substitute averaging, instead of pruning by using many fully grown trees and averaging the predictions, leading to a consistent model.  This model is very similar to bagging and differs primarily in implementation details. Also, the practical performance of these random forest variations has empirically been shown to outperform the bagging method \cite{breiman2001random}. 

The authors Chipman, George, and McCulloch (CGM) \cite{chipman1998bayesian} did further further fundamental research in developing Bayesian decision trees. In 2000 CGM formulated another modification of their previous model, this time proposing another clever prior that encouraged shrinkage and sharing of information by nodes close together in the tree \cite{chipman2000hierarchical}. Then in 2002, these authors' proposed another modification to their previous work suggesting that perhaps constant models in each terminal node were not effective or general enough to effectively model observed data. Instead, they suggested to allow GLM's in each terminal node and called the resulting models \emph{treed} models \cite{chipman2002bayesian}, which generalize classic decision tree models to include linear or generalized linear models within each terminal node. It is worth noting that the previous models proposed using constant functions are in fact special cases of treed models, they are linear regression models with only an intercept term in each terminal node. Furthermore, the amount of available data decreases the further into the tree you traverse, so either tree regularization or node model regularization is necessary to combat the ``small n, big p'' problem that occurs in terminal nodes of large trees. 

Several further refinements to the Bayesian approach were proposed during the subsequent decade. Wu, Tjelmland and West \cite{wu2007bayesian} offered an improved proposal function that contains a radical restart move that grows a new tree from scratch when the current chain is stuck in a local maximum. This is accomplished by a ``pinball prior'', which is one of the unique and practically useful aspects of the paper. As a further improvement, to aid the mixing of the Markov chain Monte Carlo and to aid the chain in traversing from one local maximum to another, Leman, Chen and Levine proposed a new MCMC algorithm called the multiset sampler \cite{leman2009multiset}. The multiset sampler is able to achieve a move from one local max to another by allowing the chain to be in two states of the Markov chain at a single instant. The multiset sampler was originally developed for evolutionary inference in the reconstruction of phylogenetic trees, however this specific application was referred to as the evolutionary forest algorithm. The extension from phylogenetic trees to CART trees is fairly straightforward.  Moreover, Gramacy and Lee extended the treed model to include Gaussian processes and gave an application to simulation of rocket boosters \cite{gramacy2008bayesian}. Gramacy and Taddy \cite{gramacy2012categorical} provide an R package that implements the ideas in Gramacy and Lee \cite{gramacy2008bayesian} and provides several extensions and special cases. 

Several other important works deserve to be mentioned during this timeframe (the 2000's).  In the computer science domain, Dobra advanced a scalable algorithm to do decision tree induction called SECRET \cite{dobra2002secret}. In addition to providing scalable algorithms for decision tree induction Dobra, offered a novel modification to decision trees: a probabilistic split at each node. This creates fuzzy classifiers and fuzzy regions in the covariate space around which splits are made. The probabilistic splits create regions of splits in the covariate space. A similar phenomenon is observed in bagged trees \cite{breiman1996bagging}, although Dobra's approach is simpler and preserves the interpretability of the single tree. Along different lines, Gray and Fan proposed genetic algorithms to build decision trees in their TARGET algorithm \cite{gray2008classification}. Genetic algorithms are roughly similar to the population approaches \cite{cappe2004population} except they rely on an analogy with evolutionary processes to guide their development.  

During the same decade (the 2000's), the stigmergic approach to decision tree induction was investigated empirically. The stigmergic approach generally works by agents, or a population approach, whereby each agent is able to communicate with other agents through the environment in which the agent acts. In the case of the ant colony optimization algorithm \cite{dorigo2004ant}, stigmergy is achieved by ants leaving pheromone trails that influence the behavior of subsequent ant agents. While ant colony optimization approaches do not generally yield the best performance in the training data, they are generally competitive with other algorithms in test data prediction and provide differing and often insightful trees \cite{dorigo2006ant}. Several authors have modified algorithms to build decision trees using the antminer system\cite{parpinelli2002data} \cite{liu2003classification}. The antminer system is publicly available in Matlab code at the link \href{http://www.antminerplus.com/}{http://www.antminerplus.com/} .   

Two additional statistical approaches have been advocated recently. The EARTH algorithm and the GUIDE algorithm \cite{doksum2008nonparametric} \cite{guide}. EARTH is an algorithm that nonparametrically selects covariates to include in the model. Doksum, Tang, and Tsui provide a theorem that shows the covariate selection consistency of the EARTH algorithm as the sample size, $n\to\infty$, and as the dimensionality of the data, $d\to\infty$. %In this respect their result is similar to one in this thesis. Nevertheless, we use different approaches to come to similar conclusions and, therefore, have different insight into the problem.  

The GUIDE algorithm \cite{guide}, proposed by Loh, is similar to the original work of Loh and Vanichsetakul \cite{loh1988tree}. The GUIDE algorithm is a general unbiased interaction detector that claims to have superior performance at detecting interactions and incorporating those into the model by splits that are linear combinations of covariates.  Unfortunately, there is no guarantee of consistency of the GUIDE algorithm.

% perhaps the following paragraph needs a few ref's to other variable importance (VIMP) work? 

Decision trees have long been applied to survival data. The explosion of cheap genome sequencing and generally cheap data collection and storage has made variable selection methods increasingly useful in the context of survival trees. The work of Ishwaran, Kogalur, Gorodeski, Minn, and Lauer defines a new quantity called a maximal subtree and use the inherent topology of the tree and the maximal subtree to measure variable importance  \cite{ishwaran2010high}. Ishwaran et al. advocate a bootstrapping approach to variable selection in the context of random survival forests. The authors noted good empirical performance and provide probability approximations to calculations necessary in their simulations. 

Finally, Taddy, Gramacy, and Polson have extended the decision tree literature into the time series domain \cite{taddy2011dynamic}. The authors propose to embed decision trees into a dynamic stochastic process. The authors suggest that the underlying tree of the model is updated by alternating grow, split, and do nothing moves. Taddy et al. also provide an illustrative example using car crash data. Time series applications of decision trees appears to be a fruitful area of research. 

