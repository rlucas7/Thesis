\chapter{Introduction}
\label{ch:intro}

This thesis outlines variable selection as a genre of research in the intersecting domains of statistics, computer science (CS) and other data sciences, including several related subfields of CS \newabbrev{abbrev:CS} and statistics. Our goal is to develop methods to perform \emph{explicit} variable selection within a decision tree model. We emphasize ``explicit'' because there are several \emph{ad hoc} methods that are currently common in applied practice. These \emph{ad hoc} methods perform variable selection using decision trees, usually with little theoretical justification, and no notion of measure on the individual variables. 

Towards our goal, we give an historical overview of decision trees, surveying literature from both CS and statistics, spanning seven decades. While an exhaustive literature review would be a Herculean task, we seek to examine a representative sample of literature to give the reader sufficient knowledge of the choices and strategies applied by researchers. 
What will emerge is a confluence of several fields including mathematics, statistics, CS and related subdomains applying their own methods of research into this diverse subdomain of study.     

We are fundamentally looking toward variable selection as the goal, but a reasonable question to ask is ``who cares?''; why should we think that variable selection is worth studying? Moreover, why should we study variable selection in this very specific subdomain of decision trees? The simplest answer is that datasets are getting bigger. With larger datasets problems of which variables to use in a model become too difficult to answer based on intuition, or domain expertise alone. Automated methods are needed. Also, cheap computing power and the march towards an interconnected web of business, socialization and life has nearly automated many data collection processes. This automation has created a 21st century gold rush into any academic pursuit that trains an individual to work with data. Additionally, interesting applied problems that were unanswerable only a few years ago may be readily answered today. 
Thus, big datasets are here to stay.

 Big data can mean a large number of observations, or a large number of variables. In this thesis we are mainly concerned with a large number of variables. Specifically, we study methods to choose subsets of variables from a decision tree model in reasonable ways. 

There are many well known statistical models, perhaps the most common is the linear model. A straight forward extension to the linear model is the transformed linear model, known as the Generalized Linear Model (hereafter abbreviated GLM). \newabbrev{abbrev:GLM} Further extensions allow for random effects, known as GLMMs \newabbrev{abbrev:GLMM}(the extra M stands for `Mixed'). The earliest developed variable selection techniques are usually considered to be the forward, backward, and stepwise techniques. These three techniques were originally studied for linear regression models in the 1960s. Examining historically, as we do in the next subsection, we will see that decision trees were also one of the first forms of variable selection when the landmark paper by Morgan and Sonquist \cite{morgan1963problems} was published. This groundbreaking work would not be fully appreciated in the research community for nearly two decades. Finally in the 1990s and the first decade of this century, the methods employed in this original paper would be in widespread use, in both academia and industry. 

This introductory chapter only outlines the material to be discussed in the subsequent thesis. We give a literature review of decision trees in the proceeding two subsections of this chapter. The final subsection of this chapter provides a brief literature review of variable selection methods. Chapter 2 gives an overview of the various decision tree models discussed in this thesis and gives some technical details about the models. Chapter 3 presents variable selection methods for decision trees using a Dirichlet type prior approach for covariate selection called the DiVaS method. In Chapter 3 we discuss a class of methods for variable selection proposed by the author of this thesis. Chapter 4 presents case studies of decision tree variable selection methods using simulated data and using data taken from the UCI \newabbrev{abbrev:UCI}machine learning repository \cite{Frank:2010uq}. Chapter 5 presents an alternative approach using normal distributions as a variable selection distribution and transforming to a probability scale using a distribution known as the additive logistic normal (ALN) \newabbrev{abbrev:ALN} distribution. We call the method that uses the ALN distribution the Additive Logistic Variable Selection (ALoVaS) method. 
This chapter shows how the class of methods from Chapter 3 can be used to solve the decision tree variable selection problem using non-Dirichlet priors for the probability of selecting a covariate. Chapter 6 presents a case study of the ALoVaS method applied to the internet ads dataset. Chapter 7 Compares  and contrasts the two methods we propose, the DiVaS \newabbrev{abbrev:DiVaS} and ALoVaS \newabbrev{abbrev:ALoVaS} methods. Finally, Chapter 8 discussed and synthesizes the results and conclusions of the thesis and points to future work. %what is a decision tree 

\section{Related Work}
\label{ch:related}


This subsection details two important aspects of the thesis: decision trees and variable selection. Little work has focused on variable selection in decision trees and, for this reason, we separate the literature review into two components. In subsequent chapters, we provide more details on the few implementations of variable selection in decision tree problems. We begin with an historical review of the decision tree literature. Then, in a subsequent subsection, we survey the literature on the variable selection problem. 

\subsection{An Historical Path Through the Literature}

In this section we review decision tree literature from an historical perspective. We begin with the earliest decision tree paper in the statistics literature, the paper by Morgan and Sonquist \cite{morgan1963problems}. Their proposal suggested that the linear model is often an inadequate method to handle complex survey data analysis. The authors outline several complications with survey data, including high correlations among the covariates, known to cause instability in linear models, and complex interactions, which make the linear model difficult to interpret and complicated to calculate. Furthermore, their paper cites only one previous author who had a similar but not the same approach. They mention an applied statistics paper from 1959 by Belson \cite{belson1959matching}, also an economist. Belson's work certainly predates Morgan and Sonquist's, but the aim of both papers appears to be to partition recursively complex survey data, applying an empirical and simple approach instead of a theoretical approach to analyzing large complicated data. 

After the pioneering work of Morgan and Sonquist and that of Belson, the next researcher to begin looking at decision tree methods of data analysis was Kass \cite{kass1975significance}. Although Kass' paper does not present a graphic of a decision tree, the method he proposes is that of recursive partitioning of the data. He gives suggestions and recommendations for how to carry out this procedure and notes the need for further research in this area. Kass follows this paper up with another paper in 1980 \cite{kass1980exploratory}. Kass \cite{kass1980exploratory} studies recursive partitioning again but this time specifically on categorical data. Conclusions and results are similar to those in Kass \cite{kass1975significance}. It is worth noting that although Breiman, Friedman, Olshen and Stone's 1984 CART \newabbrev{abbrev:CART}book \cite{breiman1984classification} is often considered the work that introduced decision trees into the statistics literature, here we have noted at least four papers prior to the 1984 publication of this book and we by no means claim to be exhaustive in our review.  Moreover, it is equally surprising that Breiman noted that they published the CART book as a book because the authors thought that no statistics journal would publish the work \cite{friedman2011remembering}.  

The work of Breiman et al. \cite{breiman1984classification} was pioneering in several aspects. The last chapter contains a theoretical proof of the consistency of decision trees as the number of observations, typically denoted $n$, grows arbitrarily large. Breiman et al. provided a new framework, which involved growing a full tree and then pruning back to optimality. This is the first instance of a consistent stopping rule in the decision tree literature. In addition, the book presents specifics of algorithmic implementation. Also many practical issues such as stopping criteria are thoroughly discussed, indicating why the full growth and then pruning approach is preferred over previously proposed simpler stopping rules. The book is an excellent reading for theoretical statisticians and applied data analysts. Moreover, the low cost of the book and the practical interpretability of decision trees made the method popular among researchers in several fields.

Other work around the same time included the pioneering work of Quinlan \cite{quinlan1986induction} and his textbook \cite{quinlan1993c4}. Both Quinlan \cite{quinlan1986induction} and Quinlan \cite{quinlan1993c4} discuss induction learning for decision trees. Quinlan's research differs from the statistical approaches in two aspects. The first fundamental differences is that Quinlan uses multiway splits compared to only binary decision tree splits in the previous statistics literature. Second, Quinlan uses an information theoretic approach to justify decision trees whereas the statistics literature takes a nonparametric approach.  

Attempting to improve the prediction errors of CART trees, Loh and Vanichsetakul proposed using linear combinations of covariates as splitting rules instead of the simple axis orthogonal splits of the CART methodology. The fundamental differences between the CART method and the work of Loh and Vanichsetakul \cite{loh1988tree} are: 

\begin{itemize}
\item Multiway, or more than two way splits are possible at each node.
\item A direct stopping rule is used. 
\item Loh and Vanichsetakul's method estimates missing values. 
\item The tree splits are linear combinations and may contain both categorical and continuous covariates.
\item Loh and Vanichsetakul's method has no invariance to monotonic transformations.
\item Computationally faster than Breiman et al.'s method. 
\end{itemize}

Furthermore, Loh and Vanichsetakul used statistical inference approaches such as $F$ ratios to choose splits and to stop the tree from growing. This work was criticized by Breiman and Friedman  \cite{breiman1988comment} on several aspects, the most important of which are the lack of robustness, no proof of a consistent stopping rule, and lack of invariance to monotonic transformations. 

In the subsequent decade much research was done, both empirically and theoretically regarding the efficacy of decision tree methods. The next major extension was done in the year 1998. Two groundbreaking papers were published, one by Chipman, George and McCulloch \cite{chipman1998bayesian}, and another by Denison, Mallick and Smith \cite{denison1998bayesian}, hereafter referred to as CGM \newabbrev{abbrev:CGM}and DMS \newabbrev{abbrev:DMS}, respectively. Both articles brought Bayesian computational techniques to bear on the problem of decision tree induction. Much Bayesian computational work was accomplished following the groundbreaking Gibbs sampler first published in 1990 by Gelfand and Smith \cite{gelfand1990sampling}, such as Metropolis-Hastings (MH)\newabbrev{abbrev:MH} samplers \cite{hastings1970monte,robert1999monte} and reversible jump methods \cite{green1995reversible}. The papers by CGM and DMS brought the 8+ years of research in Markov chain Monte Carlo (MCMC \newabbrev{abbrev:MCMC}\hspace{-.1cm}) methods into decision tree search methods. CGM proposed a novel process prior and gave a set of proposal functions that exhibit useful cancellations in the MH ratio. In a similar vein, DMS proposed a reversible jump algorithm with a similar proposal function that appears to mix more efficiently while searching the space of trees. These two papers are the genesis of our developments in the current thesis. 

Around the same time (the 1990's), the machine learning community was experiencing rapid growth. During this time of rapid growth, Leo Breiman helped to bridge the gap between the statistics community and the machine learning community, publishing fundamental work proposing the bagging method \cite{breiman1996bagging}. During the same year, 1998, Ho proposed a similar generalization known as the random subspace method \cite{ho1998random}. Both algorithms use resampling methods similar to the bootstrap \cite{efron1997improvements,efron1994introduction}, but build a decision tree on each subsampled dataset and then aggregate the predictions from the resulting trees to improve prediction and stability of the estimator. Although more refined and developed, a similar approach is the random forest model \cite{breiman2001random}. The random forest model is the subject of current research into the theoretical properties of the resulting collection of trees and their predictions \cite{biau2008consistency,biau2012analysis}. Interestingly, this research has led to the conclusion that the pruning rule makes a consistent decision tree but that an analyst may substitute averaging, instead of pruning by using many fully grown trees and averaging the predictions, leading to a consistent model.  This model is very similar to bagging and differs primarily in implementation details. Also, the practical performance of these random forest variations has empirically been shown to outperform other forms of bagging algorithms \cite{breiman2001random}. 

The authors Chipman, George, and McCulloch (CGM) \cite{chipman1998bayesian} did further fundamental research in developing Bayesian decision trees. In 2000 CGM formulated another modification of their previous model, this time proposing another clever prior that encouraged shrinkage and sharing of information by nodes close together in the tree \cite{chipman2000hierarchical}. Then in 2002, these authors' proposed another modification to their previous work suggesting that perhaps constant models in each terminal node were not effective or general enough to effectively model observed data. Instead, they suggested to allow GLM's in each terminal node and called the resulting models \emph{treed} models \cite{chipman2002bayesian}. Treed models generalize classic decision tree models to include linear or generalized linear models within each terminal node. It is worth noting that the previous models proposed using constant functions are in fact special cases of treed models. They are linear regression models with only an intercept term in each terminal node. Furthermore, the amount of available data decreases the further into the tree you traverse, so either tree regularization or node model regularization is necessary to combat the ``small n, big p'' problem that occurs in terminal nodes of large trees. 

Several further refinements to the Bayesian approach were proposed during the subsequent decade. Wu, Tjelmland and West \cite{wu2007bayesian} offered an improved proposal function that contains a radical restart move that grows a new tree from scratch when the current chain is stuck in a local maximum. This is accomplished by a ``pinball prior'', which is one of the unique and practically useful aspects of the paper. As a further improvement, to aid the mixing of the Markov chain Monte Carlo and to aid the chain in traversing from one local maximum to another, Leman, Chen and Levine proposed a new MCMC algorithm called the multiset sampler \cite{leman2009multiset}. The multiset sampler is able to achieve a move from one local max to another by allowing the chain to be in two states of the Markov chain at a single instant. The multiset sampler was originally developed for evolutionary inference in the reconstruction of phylogenetic trees, however this specific application was referred to as the evolutionary forest algorithm. The extension from phylogenetic trees to CART trees is fairly straightforward.  Moreover, Gramacy and Lee \cite{gramacy2008bayesian} extended the treed model to include Gaussian processes and gave an application to simulation of rocket boosters. Gramacy and Taddy \cite{gramacy2012categorical} provide an R package called `tgp' that implements the ideas in Gramacy and Lee \cite{gramacy2008bayesian} and provides several extensions and special cases. 

Several other important works deserve to be mentioned during this timeframe (the 2000's).  In the computer science domain, Dobra advanced a scalable algorithm to do decision tree induction called SECRET \cite{dobra2002secret}. In addition to providing scalable algorithms for decision tree induction, Dobra offered a novel modification to decision trees: a probabilistic split at each node. This creates fuzzy classifiers and fuzzy regions in the covariate space around which splits are made. The probabilistic splits create regions of splits in the covariate space. A similar phenomenon is observed in bagged trees \cite{breiman1996bagging}, although Dobra's approach is simpler and preserves the interpretability of the single tree. Along different lines, Gray and Fan proposed genetic algorithms to build decision trees in their TARGET algorithm \cite{gray2008classification}. Genetic algorithms are roughly similar to the population approaches \cite{cappe2004population} except that they rely on an analogy with evolutionary processes to guide their development.  

During the same decade (the 2000's), the stigmergic approach to decision tree induction was investigated empirically. The stigmergic approach generally works by agents, or a population approach, whereby each agent is able to communicate with other agents through the environment in which the agent acts. In the case of the ant colony optimization algorithm \cite{dorigo2004ant}, stigmergy is achieved by ants leaving pheromone trails that influence the behavior of subsequent ant agents. While ant colony optimization approaches do not generally yield the best performance in the training data, they are generally competitive with other algorithms in test data prediction and provide differing and often insightful trees \cite{dorigo2006ant}. Several authors have modified algorithms to build decision trees using the antminer system \cite{parpinelli2002data} \cite{liu2003classification}. The antminer system is publicly available in Matlab code at the link \href{http://www.antminerplus.com/}{http://www.antminerplus.com/} .   

Two additional statistical approaches have been advocated recently: the EARTH algorithm and the GUIDE algorithm \cite{doksum2008nonparametric} \cite{guide}. EARTH is an algorithm that nonparametrically selects covariates to include in the model. Doksum, Tang, and Tsui \cite{doksum2008nonparametric} provide a theorem that shows the covariate selection consistency of the EARTH algorithm as the sample size, $n\to\infty$, and as the dimensionality of the data, $d\to\infty$. %In this respect their result is similar to one in this thesis. Nevertheless, we use different approaches to come to similar conclusions and, therefore, have different insight into the problem.  

The GUIDE algorithm \cite{guide}, proposed by Loh, is similar to the original work of Loh and Vanichsetakul \cite{loh1988tree}. The GUIDE algorithm is a general unbiased interaction detector that claims to have superior performance at detecting interactions and incorporating those into the model by splits that are linear combinations of covariates.  Unfortunately, there is no guarantee of consistency of the GUIDE algorithm.

% perhaps the following paragraph needs a few ref's to other variable importance (VIMP) work? 

Decision trees have long been applied to survival data. The explosion of cheap genome sequencing and generally cheap data collection and storage has made variable selection methods increasingly useful in the context of survival trees. The work of Ishwaran, Kogalur, Gorodeski, Minn, and Lauer defines a new quantity called a maximal subtree and use the inherent topology of the tree and the maximal subtree to measure variable importance  \cite{ishwaran2010high}. Ishwaran et al. advocate a bootstrapping approach to variable selection in the context of random survival forests. The authors noted good empirical performance and provide probability approximations to calculations necessary in their simulations. 

Finally, Taddy, Gramacy, and Polson have extended the decision tree literature into the time series domain \cite{taddy2011dynamic}. The authors propose to embed decision trees into a dynamic stochastic process. The authors suggest that the underlying tree of the model is updated by alternating grow, split, and do nothing moves. Taddy et al. also provide an illustrative example using car crash data. Time series applications of decision trees appears to be a fruitful area of research. 

\subsection{A Brief Overview of Variable Selection Methods}

In order to understand the methods we will employ in further chapters, we will give a brief overview of variable selection methods for linear models. We focus on linear models because the majority of the research into variable selection has been conducted on these models. Extensions have been done for certain methods with the focus primarily on GLMs. However, while the extension is conceptually straightforward, the particulars of the algorithms are usually more complicated and specialized. Also these extensions have not been applied to all the methods we discuss. In subsequent chapters, we will see how the material we introduce here can be applied to decision trees using an appropriately defined transform. 

Perhaps the earliest variable selection method, besides the modest proposal of Morgan and Sonquist \cite{morgan1963problems}, is the forward selection method (FS).\newabbrev{abbrev:FS} The forward selection method and many variations appear in the early 1960's from several references making it very difficult to identify the person who originally proposed this method. References in other languages are not included in this review, further obfuscating the designation of first proposal. The FS method is well know to run into difficulties when several covariates  are highly correlated with each other \cite{miller1984selection}. There are several nice benefits to the FS method, such as computational feasibility and readily available, high quality computer code that implements this technique. Nonetheless, several researchers have pointed out the sometimes dubious nature of the resulting output \cite{halvorson1960regression}. 

A related method to forward search is backwards search, which operates analogously to the forward search, except the model starts with all covariates in the model. At each iteration the variable with the lowest correlation with the response is removed from the model. Also, the stepwise method devised by Efroymson \cite{efroymson1960multiple} represents a middle ground between forward and backward search by sequentially adding and deleting variables. The stepwise method tries at each step to include or exclude a variable, based upon an $F$ statistic value. The backward and stepwise searches are also known to encounter similar difficulties as FS \cite{miller2002subset} does. Highly correlated covariates may produce dubious results. A nice overview of all three methods, along with several other subset selection approaches can be found in Miller \cite{miller2002subset}. The second edition of Miller's book contains many updates, including chapters on Bayesian and regularization methods.  

 Ridge regression is another popular approach used in subset selection problems \cite{hoerl1970ridge}. The ridge regression estimator is defined as 
 \begin{equation}\label{eqn:ridge}
 \underline{\hat{\beta}}= \underset{\underline{\beta}}{argmin}\ (\underline{y} -X\underline{\beta})^T\Sigma^{-1}(\underline{y} -X^T\underline{\beta}) + \lambda\underline{\beta}^T\Gamma\underline{\beta},
 \end{equation}
  \newnot{symbol:vec}
for $\lambda\geq 0$ some scalar (constant), with $\vec{y}$ an $n \times 1$ vector, $\vec{\beta}$ a $d\times 1$ vector and $X$ an $n \times d $ matrix. The objective function (\ref{eqn:ridge}) has the closed form solution $\underline{\hat{\beta}} = (X^TX+\lambda\Gamma)^{-1}X^T\underline{y}$ and $\Gamma$ is a matrix chosen to be conformable for addition with $X^TX$. Ridge regression combats the effect of high correlation among the columns of $X$, allowing the matrix ($X^TX$) to be inverted. This method is often most useful when $d<n$ and regression coefficients are desired. Note that estimated $\hat{\underline{\beta}}$ vectors with zero entries are unattainable in this penalized regression method when $\Gamma$ is a full rank matrix.
 
 Frank and Friedman \cite{lldiko1993statistical} proposed another penalized regression approach to ridge regression called bridge regression. Frank and Friedman also gave an optimization algorithm that solves the objective function. 
 The objective function to solve was defined as
 \begin{equation}
\hat{\underline{\beta}}= \underset{\underline{\beta}}{argmin}\ \ (\underline{y} -X\underline{\beta})^T\Sigma^{-1}(\underline{y} -X^T\underline{\beta}) + \lambda\vert\underline{\beta}\vert_{\nu}^{\nu},
  \end{equation}  
where the second term denotes a $\nu$ norm \newnot{symbol:norm} and $\nu \geq 0$ is a specified constant. This technique later became known as ``bridge'' regression, because the objective function bridges between several well known estimators by choosing various values of $\nu$. For example $\nu = 0,1,2$ correspond to subset selection, the lasso, and ridge regression, respectively. We note that the $\nu=0$ case is interpreted as $\lim_{\nu\to0}\vert\underline{\beta}\vert_{\nu}^{\nu} = \vert\underline{\beta}\vert_0$, the limiting case of the $\nu$-norm where the norm counts the number of non-zero elements of the vector. 
 
 During the 1990's Bayesian approaches became practical because of advances in computational statistics, especially developments in Gibbs sampling and MH algorithms. These advances lead to several researchers proposing Bayesian variable selection techniques. The first of these advances in the variable selection literature was the stochastic search variable selection (SSVS)\newabbrev{abbrev:SSVS} approach of George and McCulloch \cite{george1993variable}.  The SSVS approach relies on  \newnot{symbol:ind_A}
 \begin{equation}
  \delta(x_0) = \lim_{\sigma\to 0}\phi(x_0;\sigma) = d\mathds{1}[x\geq x_0],
 \end{equation}
where $\phi(a;b)$ denotes a Gaussian density evaluated at the point $a$, with standard deviation $b$ and mean zero \cite{geweke1996variable,mitchell1988bayesian}. The notation $ \delta(x_0)$ will be used to denote the Dirac delta functional. Essentially we are trying to determine if $\beta_j=0$, or if $\beta_j\neq0$ and we might wish to assign a point mass probability to $\beta_j=0$. Thus, a reasonable approximation is to use a two component mixture of normal distributions, with one normal having a large variance compared to the other. Using a latent variable representation, George and McCulloch gave a Gibbs sampling algorithm that samples subsets of predictors and thereby provides variable selections.  The main drawback is that George and McCulloch only offered SSVS for the Gaussian linear model. Extensions in the literature indicate the method can be applied to GLMs and to problems where the number of covariates is larger than the number of observations, also called the `$p>n$ problem', such as gene selection \cite{yi2003stochastic,george2000variable}.  
     
Historically, the 1990s were a fruitful period of research and methods that were proposed earlier but were not yet computationally feasible were rediscovered. Breiman advocated better subset selection using the nonnegative garrote \cite{breiman1995better,yuan2007non}. As indicated by the title, Breiman's procedure \cite{breiman1995better} selected better subsets compared to backward search and subset selection. The subsets selected are the non zero values of the estimated coefficients, conventionally denoted as $\underline{\hat{\beta}}$. In the non-negative garrote problem these estimates of $\underline{\beta}$ are obtained by solving the objective function 
\begin{equation}\label{eqn:nn_garrote}
\underset{\forall j: c_j\geq 0}{argmin}\ \ \sum_{i=1}^n(y_i -\sum_{j=1}^dc_j\hat{\beta_j}x_{ij})^2 + \lambda\sum_{j=1}^dc_j,
\end{equation}    
where $\lambda$ is a specified constant, or estimated by some other means. The estimate $\hat{\beta_j}$ denotes the $j$th least squares estimate. Alternately we can use an estimate of $\beta_j$ obtained by means other than least squares, for example using a ridge estimator. Once $\hat{c}_j$ is estimated, the non-negative garrote estimated is $\hat{\beta}_j^{\text{NNG}} =\hat{c}_j\hat{\beta}_j^{\text{LS}}$. Under an orthogonal covariate assumption ($X^TX=I$), there are closed form solutions to the objective function (\ref{eqn:nn_garrote}) that have nice interpretations as hard thresholding rules. The threshold is determined as a function of the Lagrange multiplier $\lambda$. For completeness, we derive these threshold formulae in Appendix B. 
Breiman compared the non-negative garrote method against subset selection and backward search procedures, indicating positive results for the non-negative garrote. Yuan and Lin \cite{yuan2007non} and Xiong \cite{xiong2010some} give further results for the non-negative garrote. Yuan and Lin used the theory of duality to give oracle type properties of the nonnegative garrote estimator. Briefly, the oracle property states that $\Pr(\hat{\beta} = \hat{\beta}_{\text{global}})\to 1$ as $\lambda_n\to\lambda$. In other words, this means the local estimator becomes the global estimator for a suitably constructed sequence of regularization parameters. Xiong studied iterating the non-negative garrote procedure and also how the degrees of freedom influence the prediction risk of estimates.    

We now move to discuss Bayesian approaches to variable selection. In the Bayesian approach, the constraint in the optimization problem is motivated as the negative logarithm of a prior measure placed on the parameter(s) of interest. We start first in the context of sampling methods. Besides the method proposed by George and McCulloch \cite{george1993variable}, there is another popular method applied to variable selection problems in a Bayesian context. This alternate method is known as reversible jump Markov chain Monte Carlo (RJ-MCMC).\newabbrev{abbrev:RJ-MCMC} This method was first suggested by Green \cite{green1995reversible}. Green showed that mixtures of normal distributions can be modeled using the RJ-MCMC sampler. Specifically, the RJ-MCMC algorithm eliminates the need to specify the number of mixtures in the normal distribution, effectively eliminating tuning parameters from normal mixture distribution problems.

One of the most fruitful areas of research in the last 15 years has been the lasso,\newabbrev{abbrev:LASSO} which stands for \emph{least absolute selection and shrinkage operator}. Motivated by the non-negative garrotte, the lasso was proposed by Tibshirani in 1996 \cite{tibshirani1996regression}. Many subsequent papers give properties of the lasso, or proposed alternate methods to solve the objective function. The objective function is defined as
\begin{equation}\label{eqn:lasso_obj}
\underset{\underline{\beta}}{argmin}\ \sum_{i=1}^n(y_i - \sum_{j=1}^dx_{ij}\beta_j)^2 +\lambda\sum_{j=1}^d\vert \beta_j\vert_1,
\end{equation}
 where the $\lambda \geq 0$, is a constant or tuning parameter. Tibshirani solved the optimization by doing a grid search across several values of $\lambda$. Unhappy with the computational complexity of the linearly constrained quadratic programming optimization approach suggested by Tibshirani \cite{tibshirani1996regression}, Efron et al. \cite{efron2004least} proposed the least angle regression algorithm, hereafter referred to as LAR,\newabbrev{abbrev:LAR} to solve the lasso optimization problem. The LAR algorithm uses a homotopy method to solve the objective function, Equation \ref{eqn:lasso_obj}. Along similar lines, both Osborne, Presnell and Turlach, and Zhao and Yu used duality theory to prove properties of the lasso estimators \cite{osborne2000lasso,zhao2007model}. Bunea, Tsybakov, and Wegkamp  \cite{bunea2007sparsity} and Candes and Plan  \cite{candes2011tight} have also derived oracle and optimality properties of the lasso problem estimators. These optimality results state that lasso solutions are within a constant factor of the true values, with the constant usually being a number of the form $1+\epsilon$ and $1\geq\epsilon\geq 0$. Tibshirani \cite{tibshirani1996regression} originally noted a Bayesian approach to interpreting Equation \ref{eqn:lasso_obj}. Park and Casella  \cite{park2008bayesian} gave further Bayesian lasso results including a marginal maximum likelihood (empirical Bayes) method for estimating the tuning parameter $\lambda$. An empirical Bayes argument is used to justify this method of estimating $\lambda$. The primary result used by Park and Casella is that a Laplace prior can be written as an exponential scale mixture of a normal random variable. The exponential scale mixture result is formally derived for completeness in Appendix C using moment generating functions .  Finally, Zhou and Hastie \cite{zou2005regularization} combined the penalties of the lasso and of ridge regression and called this objective function the elastic net. The elastic net can be interpreted as a convex combination of a lasso ($\vert \underline{\beta}\vert_1$) penalty and a ridge regression ($\vert \underline{\beta}\vert_2^2$) penalty. Zhou and Hastie \cite{zou2005regularization} provided a transformation to convert the elastic net problem into a lasso problem so that the LAR algorithm can be used to solve the elastic net objective function efficiently.      
 
The flurry of regularization papers on the lasso and the non-negative garrote methods inspired Candes and Tao \cite{candes2007dantzig}. Candes and Tao \cite{candes2007dantzig} advocated an alternate method of estimating regressors called the Dantzig selector. The Dantzig selector estimates a sparse vector of coefficients, denoted $\hat{\underline{\beta}}^{\text{D}}$, by solving the objective function
\begin{equation}\label{eqn:dantzig_sel}
\underset{\underline{\beta}}{argmin}\ \vert \underline{\beta}\vert_1 + \lambda\vert X^T(\underline{y} -X\underline{\beta}) -k_p\sigma\vert_{\infty}.
 \end{equation}
Candes and Tao \cite{candes2007dantzig} suggested that this objective function be reformulated as a linear program. Linear programs have several highly reliable software applications to estimate the optimum. Candes and Tao \cite{candes2007dantzig} also gave a primal-dual interior point algorithm to solve the objective function with publicly available Matlab code. The authors emphasized a uniform uncertainty principle (UUP)\newabbrev{abbrev:UUP} and derived oracle and optimality results based on the UUP. Bickel, Ritov, and Tsybakov \cite{bickel2009simultaneous} and Koltchinksii \cite{koltchinskii2009dantzig} provided further theoretical analysis of the Dantzig selector, including optimality results and oracle inequalities under different conditions than Candes and Tao \cite{candes2007dantzig}.

 The last estimator we discuss is the horseshoe estimator, arising from the horseshoe prior. This prior was proposed by Gelman as a way to combat a numerical difficulty in MCMC sampling \cite{gelman2006prior}. Carvalho, Polson, and Scott \cite{carvalho2010horseshoe,carvalhohandling} describe the general setup. The horseshoe estimator arises via the hierarchical probability representation described in Equations \ref{eqn:horseshoe1}-\ref{eqn:horseshoe4} beneath 
 \begin{equation}\label{eqn:horseshoe1}
 \underline{y}\vert\underline{\beta} \sim N(X\underline{\beta}, \sigma^2I),
 \end{equation}  
 \begin{equation}\label{eqn:horseshoe2}
 \beta_j\vert\lambda_j,\tau \sim N(0, \lambda_j^2\tau^2),
 \end{equation}  
  \begin{equation}\label{eqn:horseshoe3}
  \pi(\lambda_j)\propto \frac{1}{1+\lambda_j^2}\mathds{1}[\lambda_j\geq0], \text{ and}
 \end{equation} 
   \begin{equation}\label{eqn:horseshoe4}
  \pi(\tau)\propto \frac{1}{1+\tau^2}\mathds{1}[\tau\geq0].
 \end{equation}   \newnot{symbol:sim} \newnot{symbol:normal} \newnot{symbol:propto}
Here it should be made clear that the horseshoe prior is a half-Cauchy distribution. It is worth noting that the local scale priors $\lambda_j$ for $j=1, \dots, d$ are on the standard deviation scale and this prior is one of a general class of half-t densities \cite{gelman2006prior,polson2011half}. Carvalho et al. \cite{carvalhohandling} showed that the horseshoe prior has several appealing properties, including sparsity properties shared by the lasso estimator, while also having other desirable properties \emph{not} shared by the lasso. Carvalho et al. \cite{carvalho2010horseshoe,carvalhohandling} gave examples demonstrating the utility of this method for variable selection and indicated superior performance compared to the lasso in the datasets examined. Moreover, Polson \cite{polson2011half} argued that the half-t prior, or the half Cauchy prior as a special case, should become the reference (default) prior for variable selection problems. Polson justified this argument based on the many desirable properties of the horseshoe, some of which are not shared with other estimators such as the lasso. 
 
  The attentive reader may notice that the majority of material in this subsection pertains to linear regression models. Decision tree models are inherently nonlinear in nature so it is reasonable to wonder: Will we use any of the material discussed in this section? The short answer is ``yes,'' but not directly. Our approach will be in terms of linking these sparse linear models with decision trees and variable selections. The ALoVaS method provides exactly this link and we will be elaborate in subsequent chapters. We explain our decision tree model in the next chapter and show how we exploit sparsity in said model by using modifications to distributions to encourage sparsity. 
  
 We conclude this section by noting some review papers on the variable selection problem within the literature. O'Hara and Silanp\"{a}\"{a} \cite{o2009review} gave a recent notable Bayesian survey. This paper covered many of the methods discussed above in some detail and compared them all from a Bayesian perspective. Miller \cite{miller2002subset} gave a book length treatment on variable selection methods, with the second edition of the book including some Bayesian methods and the lasso, but not all methods discussed in this section. In particular, Miller \cite{miller2002subset} described the forward, backward and stepwise searches in detail and gave several useful references to the literature. From the CS literature, the review by Dash and Liu  \cite{dash1997feature} provided a useful reference into the CS field developments in variable selection. Dash and Liu \cite{dash1997feature} also provided a useful framework to compare subset/variable selection approaches. George \cite{george2000variable} provided a good overview and major references in the variable or subset selection field current to the year 2000.     

 \newpage
 