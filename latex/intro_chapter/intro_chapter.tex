\section{Introduction}
\label{ch:intro}

%Include an introductory paragraph here befroe the start of section 1.1.

% Citing previous \cite{breiman1984classification} and current  \cite{gramacy2012categorical} work

%\subsection{The Scope and Nature of This Thesis}

%list/outline 

% what is the goal of this thesis? 

This thesis outlines variable selection as a genre of research in the intersecting domains of statistics, computer science (CS) and other data sciences, including several related subfields of CS \newabbrev{abbrev:CS} and statistics. Our goal is to develop methods to perform \emph{explicit} variable selection within a decision tree model. We emphasize ``explicit'' because there are several \emph{ad hoc} methods that are currently common in applied practice. These \emph{ad hoc} methods perform variable selection using decision trees, usually with little theoretical justification, and no notion of measure on the individual variables. 

Towards our goal, we give an historical overview of decision trees, surveying literature from both CS and statistics, spanning seven decades. While an exhaustive literature review would be a Herculean task, we seek to examine a representative sample of literature to give the reader sufficient knowledge of the choices and strategies applied by researchers. 
What will emerge is a confluence of several fields including mathematics, statistics, CS and related subdomains applying their own methods of research into this diverse subdomain of study.     

% traditional role of variable selection.

We are fundamentally looking toward variable selection as the goal, but a reasonable question to ask is ``who cares?''; why should we think that variable selection is worth studying? Moreover, why should we study variable selection in this very specific subdomain of decision trees? The simplest answer is that datasets are getting bigger. Cheap computing power and the march towards an interconnected web of business, socialization and life has nearly automated many data collection processes. This automation has created a 21st century gold rush into any academic pursuit that trains an individual to work with data. %In addition, capital investments have followed recognizing the potential for overnight wealth creation with the properly chosen and implemented idea.
Thus, big datasets are here to stay.

 Big data can mean a large number of observations, or a large number of variables. In this thesis we are mainly concerned with a large number of variables. Specifically, we study methods to choose subsets of variables from a decision tree model in reasonable ways. 

% methods of VS: forward, backward, stepwise<- ?are these the same? -> stagewise, SSVS: spike-slab etc. Geweke's approach, RJMCMC, any I missed? 

There are many well known data models, perhaps the most common is the linear model. A straight forward extension to the linear model is the transformed linear model, known as the generalized linear model (hereafter abbreviated GLM). \newabbrev{abbrev:GLM} Further extensions allow for random effects, known as GLMMs \newabbrev{abbrev:GLMM}(the extra M stands for `mixed'). The earliest developed variable selection techniques are usually considered to be the forward, backward, and stepwise techniques. These three techniques were originally studied for linear regression models in the 1960s. Examining historically, as we do in the next subsection, we will see that decision trees were also one of the first forms of variable selection when the landmark paper by Morgan and Sonquist \cite{morgan1963problems} was published. This groundbreaking work would not be fully appreciated in the research community for nearly two decades. Finally in the 1990s and the first decade of this century, the methods employed in this original paper would be in widespread use, in both academia and industry. 

% straightforward implementation for linear models, GLM and GLMM are straightforward extensions. How to accomplish VS in context of decision trees? 

%One of the primary benefits of the linear model is stated within its' name \emph{linearity}. Linearity makes conceptualizing, computation, and interpretation simple. The original benefit of the theory of the GLM model was that codes previously used for weighted least squares could be used to fit GLM models with relatively little change. An additional benefit of the GLM model is the linearity in the transformed space. This linearity allows an analyst to apply all the variable selection methods from the linear model to the GLM (and GLMM) models. Contrast this with the decision tree model where there is no linearity in the tree model. The recursive partitioning model was proposed initially to handle complex survey data \cite{morgan1963problems} and deal with interactions that the linear model cannot handle or would be too complicated to practically handle. As with most things this model change represents a compromise between simplicity of the model and simplicity of the computational procedure that returns the estimator. 

This introductory chapter only outlines the material to be discussed in the subsequent thesis. We give a literature review of decision trees in the proceeding two subsections of this chapter. The final subsection of this chapter provides a brief literature review of variable selection methods. Chapter 2 gives an overview of the various decision tree models discussed in this thesis and gives some technical details about the models. Chapter 3 presents variable selection methods for decision trees using a Dirichlet prior type prior approach for covariate selection. This chapter includes a class of methods for variable selection proposed by the author of this thesis. Chapter 4 presents case studies of decision tree variable selection methods using simulated data and using data taken from the UCI \newabbrev{abbrev:UCI}machine learning repository \cite{Frank:2010uq}. Chapter 5 presents an alternative approach using normal distributions as a variable selection distribution and transforming to a probability scale using a transform we call the ALN \newabbrev{abbrev:ALN}transform.
This chapter shows how the class of methods from Chapter 3 can be used to solve the decision tree variable selection problem using non-Dirichlet priors for the probability of selecting a covariate. Chapter 6 outlines the research I propose to conduct during my remaining time at Virginia Tech. This proposed research includes four regularization type approaches to variable selection in decision trees. Each proposed approach is briefly outlined over 1-2 pages. Chapter 7 provides a tentative timeline for the completion of this proposed research. 
%what is a decision tree (briefly)

%how has VS been incorporated previously (briefly)

%How will the rest of the thesis be structured? 

\subsection{Related Work}
\label{ch:related}

%
%	What do I want to say with this chapter? Decision trees are useful tools at the boundary of Stats, dataMining, and CS 
%
%	Emphasize History? 
%	Morgan&Sonquist -> CART book -> C4.5 -> Bayes CART (CGM and DMS)-> TGP -> tgp Rpackage -> Dynamic Trees -> population approaches ->genetic algs ->swarm algs
%
% Not sure where these fit: SECRET (alin dobra) and probabilistic dtrees (alin dobra phd thesis), Loh's guide program,   
%
% alternate approach: research in stats:  one timeline and parallel timeline: research in CS 
%
%%CS historical path: BDD 1959 paper, C4.5 quinlan (1986), SECRET (dobra ~2001), Scaling machine learning paper fur websearch/query, Swarm ACO
%
%%STATS path: 1963 morgan sonquist, 1984 CART, Bayes (CGM and DMS), TARGET (genetic algorithsm), TGP (GL and TG),  Dynamic Trees (TG)
%
%This part talks about related work of my proposal. This section contains both the introduction and literature review.   
%
%At least 30-40 citations. Importance of the work. To include: morgan \& sonquist 1963, CART book (BFOS), C4.5 book (Quinlan), Bayes cart (CGM and DMS), Treed gaussian processes GL, Taddy and Gramacy tgp R package, Dynamic trees Taddy and Gramacy, Genetic algorithms (TARGET algorithm). Swarm optimizations for decision trees refs?. 
%
%\textbf{Parts of the intro:} CS historical path, STATS historical path, variable selection Historical path-> both CS and STATS... 

%\subsubsection{A Historical Path Through the Computer Science Literature}


%\subsubsection{A Historical Path Through the Statistics Literature}
\input{intro_chapter/stat_lit_review/stat_lit_review.tex}
%outline for this section: 
%P0: introduction
%P1: morgan and sonquist 1963
%P2:Kass 1980 paper 
%P3: Breiman et al 1984
%P4: Wei-Yin Loh & Vanichsetakul 1987 
%P5: Bayes -> CGM(1998) + DMS 
%P6: 1999 + 2004 papers by Shih on split selection rules
%P6: further work of CGM(2001, 2003)
%P7: Loh's guide algorithm 2008
%P8: Kjell doksum, Nonparametric variable selection: the EARTH algorithm (2008 JASA)
%P6: Gaussian Processes GL (2008)
%P7: Dynamic Trees (2012) TG, and the tgp R package
%P :Loh 2012 variable selection in decision trees lecture notes in stats article
%P8: conclusion  

\subsubsection{A Brief Overview of Variable Selection Methods}

\input{intro_chapter/variable_selection_section_intro/var_select_section_intro.tex}

% outline for this section 
%P0: intro 
%P1: Earliest variable selection (besides Morgan and sonquist paper)
% P2: Forward selection 
%P3: Backward selection 
%P4: stepwise selection 
%P4.5: include ridge regression??
%P5: Bridge regression Frank and Freidman 1993 technometrics
%P5.25 SSVS george and Mcculloch1993 JASA (also -> Ed george 2000 JASA the variable selection problem) and DELLAPORTAS et al 2002 stats&computing
%P5.5: The nonnegative garrotte technometrics 1995
%P5.75 Peter Green RJMCMC for variable selection
%P6: The Lasso Tibshirani 1996
% P7: Further generalizations. Maybe 2 Paragraphs... 
%P8:  Conclusions... 
 
 %Include??: Kim, S., Tadesse, M. G., & Vannucci, M. (2006). Variable selection in clustering via Dirichlet process mixture models. Biometrika, 93(4), 877-893.
 % Also include??: O'Hara, R. B., & SillanpŠŠ, M. J. (2009). A review of Bayesian variable selection methods: what, how and which. Bayesian Analysis, 4(1), 85-117. 

%look through the new version of "subset selection" by Miller cite-> Miller, A. (2002). Subset selection in regression. Chapman & Hall/CRC.

