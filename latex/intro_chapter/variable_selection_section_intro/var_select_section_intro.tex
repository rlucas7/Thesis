% outline for this section 
%P0: intro 
%P1: Earliest variable selection (besides Morgan and sonquist paper)
% P2: Forward selection 
%P3: Backward selection 
%P4: stepwise selection 
%P4.5: include ridge regression??
%P5: Bridge regression Frank and Freidman 1993 technometrics
%P5.25 SSVS george and Mcculloch1993 JASA (also -> Ed george 2000 JASA the variable selection problem) and DELLAPORTAS et al 2002 stats&computing
%P5.5: The nonnegative garrotte technometrics 1995
%P5.75 Peter Green RJMCMC for variable selection
%P6: The Lasso Tibshirani 1996
% P7: Further generalizations. Maybe 2 Paragraphs... 
%P8:  Conclusions... 
 

%%%% --------------------INTRO PARAGRAPH PROVIDING FRAMEWORK FOR THE VARIOUS METHODS-----------------------------

In order to understand the methods we will employ in further chapters, we will give a brief overview of variable selection methods for linear models. We focus on linear models because the majority of the research into variable selection has been conducted on these models. Of course extensions have been done for certain methods on GLMs, however this material is usually more complicated and specialized. Also these extensions have not been applied to all the methods we discuss. In subsequent chapters, we will see how the material we introduce here can be applied to decision trees using an appropriately defined transform. 

Perhaps the earliest variable selection method, besides the modest proposal of Morgan and Sonquist \cite{morgan1963problems}, is the forward selection method (FS).\newabbrev{abbrev:FS} %The FS method starts with the set of variables that is the empty set denoted $\varnothing$. The current set of variables entered into the model will be denoted $\mathcal{E}, \mathscr{E}$. 
%Outline the algorithm for forward selection
%So initially $\mathscr{E} \equiv \varnothing$. Then the correlation between $\underline{y}$ and $\underline{X}_i$ is calculated for each $i \in \{1:d\}$ where $d$ is the total number of covariates (dimensions), denote these estimated correlations $\hat{\rho}_i$. The largest empirical correlation is then entered into the model, formally enter $j_1=\underset{i}{argmax}\hat{\rho}_i$ and then set $\mathscr{E} = \{j_1\}$ and $i \leftarrow i \setminus j_1$. Then the responses $\underline{y}$ are transformed to be orthogonal to the current set of predictors, in this iteration this is just $j_1$. To accomplish this we take the orthogonal projection $I-P_{j_1} = I-X_{j_1}( X_{j_1}^T X_{j_1})^{-1} X_{j_1}^T$ and set $\underline{y}^\prime = \underline{y}(I-P_{j_1})$. The steps from correlation estimation through to the current step are now done on the new orthogonal response $\underline{y}^\prime$, with the process continuing until all variables are entered or a predefined stopping rule is satisfied. 
The forward selection method and many variations appear in the early 1960's from several references making it very difficult to identify the person who originally proposed this method. References in other languages are not included in this review, further obfuscating the designation of first proposal. The FS method is well know to run into difficulties when several covariates  are highly correlated with each other \cite{miller1984selection}. There are several nice benefits to the FS method, such as computational feasibility and readily available, high quality computer codes that implement this technique. Nonetheless, several researchers have pointed out the sometimes dubious nature of the resultant output \cite{halvorson1960regression}. 

A related method to forward search is backwards search, which operates analogously to the forward search, except the model starts with all covariates in the model. At each iteration the variable with the lowest correlation with the response is removed from the model. Also, the stepwise method devised by Efroymson \cite{efroymson1960multiple} represents a middle ground between forward and backward search by sequentially adding and deleting variables. The stepwise method tries at each step to include or exclude a variable, based upon an $F$ statistic value. The backward and stepwise searches are also known to encounter similar difficulties as FS \cite{miller2002subset} does. Highly correlated covariates may produce dubious results. A nice overview of all three methods, along with several other subset selection approaches can be found in Miller \cite{miller2002subset}. The second edition of Miller's book contains many updates, including chapters on Bayesian and regularization methods.  

 Ridge regression is another popular approach used in subset selection problems \cite{hoerl1970ridge}. The ridge regression estimator is defined as 
 \begin{equation}\label{eqn:ridge}
 \underline{\hat{\beta}}= \underset{\underline{\beta}}{argmin}\ (\underline{y} -X\underline{\beta})^T\Sigma^{-1}(\underline{y} -X^T\underline{\beta}) + \lambda\underline{\beta}^T\Gamma\underline{\beta},
 \end{equation}
  \newnot{symbol:vec}
for $\lambda\geq 0$ some scalar (constant). The objective function (\ref{eqn:ridge}) has the closed form solution $\underline{\hat{\beta}} = (X^TX+\lambda\Gamma)^{-1}X^T\underline{y}$ and $\Gamma$ is a matrix chosen to be conformable for addition with $X^TX$. Ridge regression combats the effect of high correlation among the columns of $X$, allowing the matrix ($X^TX$) to be inverted. This method is often most useful when $d<n$ and regression coefficients are desired. Note that estimated $\hat{\underline{\beta}}$ vectors with zero entries are unattainable in this penalized regression method when $\Gamma$ is a full rank matrix.  % cite Hoerl, A. E., & Kennard, R. W. (1970). Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1), 55-67.      
 
 Frank and Friedman \cite{lldiko1993statistical} proposed another penalized regression approach to ridge regression called bridge regression. Frank and Friedman also gave an optimization algorithm that solves the objective function. % the ridge regression estimator and includes as special cases: the ridge estimator, the lasso, and the subset selection problem.
 The objective function to solve was defined as
 \begin{equation}
\hat{\underline{\beta}}= \underset{\underline{\beta}}{argmin}\ \ (\underline{y} -X\underline{\beta})^T\Sigma^{-1}(\underline{y} -X^T\underline{\beta}) + \lambda\vert\underline{\beta}\vert_{\nu}^{\nu},
  \end{equation}  
where the second term denotes a $\nu$ norm \newnot{symbol:norm} and $\nu \geq 0$ is a specified constant. This technique later became known as ``bridge'' regression, because the objective function bridges between several well known estimators by choosing various values of $\nu$. For example $\nu = 0,1,2$ correspond to subset selection, the lasso, and ridge regression, respectively. We note that the $\nu=0$ case is interpreted as $\lim_{\nu\to0}\vert\underline{\beta}\vert_{\nu}^{\nu} = \vert\underline{\beta}\vert_0$, the limiting case of the $\nu$-norm. 
 
 During the 1990's Bayesian approaches became practical because of advances in computational statistics, especially developments in Gibbs sampling and MH algorithms. These advances lead to several researchers proposing Bayesian variable selection techniques. The first of these advances in the variable selection literature was the stochastic search variable selection (SSVS)\newabbrev{abbrev:SSVS} approach of George and McCulloch \cite{george1993variable}.  The SSVS approach relies on  \newnot{symbol:ind_A}
 \begin{equation}
  \delta(x_0) = \lim_{\sigma\to 0}\phi(x_0;\sigma) = d\mathds{1}[x\geq x_0],
 \end{equation}
where $\phi(a;b)$ denotes a Gaussian density evaluated at the point $a$, with standard deviation $b$ and mean zero \cite{geweke1996variable,mitchell1988bayesian}. The notation $ \delta(x_0)$ will be used to denote the Dirac delta functional. Essentially we are trying to determine if $\beta_j=0$, or if $\beta_j\neq0$ and we might wish to assign a point mass probability to $\beta_j=0$. Thus, a reasonable approximation is to use a two component mixture of normal distributions, with one normal having a large variance compared to the other. Using a latent variable representation, George and McCulloch also gave a Gibbs sampling algorithm that samples subsets of predictors and thereby provides variable selections.  The main drawback is that George and McCulloch only offered SSVS for the Gaussian linear model. Extensions in the literature indicate the method can be applied to GLMs and to problems where the number of covariates is larger than the number of observations, such as gene selection \cite{yi2003stochastic,george2000variable}.  
     
Historically, the 1990s was a fruitful period of research and often rediscovery of methods proposed earlier but were not yet computationally feasible. Breiman advocated better subset selection using the nonnegative garrote \cite{breiman1995better,yuan2007non}. As indicated by the title, Breiman's procedure \cite{breiman1995better} selected better subsets compared to backward search and subset selection. The subsets selected are the non zero values of the estimated coefficients, conventionally denoted as $\underline{\hat{\beta}}$. In the non-negative garrote problem these estimates of $\underline{\beta}$ are obtained by solving the objective function 
\begin{equation}\label{eqn:nn_garrote}
\underset{\forall j: c_j\geq 0}{argmin}\ \ \sum_{i=1}^n(y_i -\sum_{j=1}^dc_j\hat{\beta_j}x_{ij})^2 + \lambda\sum_{j=1}^dc_j,
\end{equation}    
where $\lambda$ is a specified constant, or estimated by some other means. The estimate $\hat{\beta_j}$ denotes the $j$th least squares estimate. Alternately we can use a estimate of $\beta$ obtained by means other than least squares. Once $\hat{c}_j$ is estimated, the non-negative garrote estimated is defined as $\hat{\beta}_j^{\text{NNG}} =\hat{c}_j\hat{\beta}_j^{\text{LS}}$. Under an orthogonal covariate assumption ($X^TX=I$), there are closed form solutions to the objective function (\ref{eqn:nn_garrote}) that have nice interpretations as hard thresholding rules. The threshold is determined as a function of the constraint multiplier $\lambda$. %These threshold formulae are derived in an Appendix A. 
Breiman compared the non-negative garrote method against subset selection and backward search procedures, indicating positive results for the non-negative garrote. Yuan and Lin \cite{yuan2007non} and Xiong \cite{xiong2010some} give further results for the non-negative garrote. Yuan and Lin used the theory of duality to give oracle type properties of the nonnegative garrote estimator. Briefly, the oracle property states that $\Pr(\hat{\beta} = \hat{\beta}_{\text{global}})\to 1$ as $\lambda_n\to\lambda$. In other words, this means the local estimator becomes the global estimator for a suitably constructed sequence of regularization parameters. Xiong studied iterating the non-negative garrote procedure and also how the degrees of freedom influence the prediction risk of estimates.    

We now move to discuss a Bayesian approach to variable selection in the context of sampling methods. Besides the method proposed by George and McCulloch \cite{george1993variable}, there is another popular method applied to variable selection problems in a Bayesian context. This alternate method is known as reversible jump Markov chain Monte Carlo (RJ-MCMC).\newabbrev{abbrev:RJ-MCMC} This method was first suggested by Green \cite{green1995reversible}. Green showed that mixtures of normal distributions can be modeled using the RJ-MCMC sampler. Specifically, the RJ-MCMC algorithm eliminates the need to specify the number of mixtures in the normal distribution, effectively eliminating tuning parameters from normal mixture distribution problems.

One of the most fruitful areas of research in the last 15 years has been the lasso,\newabbrev{abbrev:LASSO} which stands for \emph{least absolute selection and shrinkage operator}. Motivated by the non-negative garrotte, the lasso was proposed by Tibshirani in 1996 \cite{tibshirani1996regression}. Many subsequent papers give properties of the lasso, or proposed alternate methods to solve the objective function. The objective function is defined as
\begin{equation}\label{eqn:lasso_obj}
\underset{\underline{\beta}}{argmin}\ \sum_{i=1}^n(y_i - \sum_{j=1}^dx_{ij}\beta_j)^2 +\lambda\sum_{j=1}^d\vert \beta_j\vert_1,
\end{equation}
 where the $\lambda \geq 0$, is a constant or tuning parameter. Tibshirani solved the optimization by doing a grid search across several values of $\lambda$. Unhappy with the computational complexity of the linearly constrained quadratic programming optimization approach suggested by Tibshirani \cite{tibshirani1996regression}, Efron et al. \cite{efron2004least} proposed the least angle regression algorithm, hereafter referred to as LAR,\newabbrev{abbrev:LAR} to solve the lasso optimization problem. The LAR algorithm uses a homotopy method to solve the objective function (Equation \ref{eqn:lasso_obj}). Along similar lines, both Osborne, Presnell and Turlach, and Zhao and Yu used duality theory to prove properties of the lasso estimators \cite{osborne2000lasso,zhao2007model}. Bunea, Tsybakov, and Wegkamp  \cite{bunea2007sparsity} and Candes and Plan  \cite{candes2011tight} have also derived oracle and optimality properties of the lasso problem estimators. These optimality results state that lasso solutions are within a constant factor of the true values, with the constant usually being a number of the form $1+\epsilon$ and $1\geq\epsilon\geq 0$. Tibshirani \cite{tibshirani1996regression} originally noted a Bayesian approach to interpreting Equation \ref{eqn:lasso_obj}. Park and Casella  \cite{park2008bayesian} gave further Bayesian lasso results including a marginal maximum likelihood (empirical Bayes) method for estimating the tuning parameter $\lambda$. An empirical Bayes argument is used to justify this method of estimating $\lambda$. Finally, Zhou and Hastie \cite{zou2005regularization} combined the penalties of the lasso and of ridge regression and called this objective function the elastic net. The elastic net can be interpreted as a convex combination of a lasso ($\vert \underline{\beta}\vert_1$) penalty and a ridge regression ($\vert \underline{\beta}\vert_2^2$) penalty. Zhou and Hastie \cite{zou2005regularization} provided a transformation to convert the elastic net problem into a lasso problem so that the LARs algorithm can be used to solve the elastic net objective function efficiently.      
 
The flurry of regularization papers on the lasso and the non-negative garrote methods inspired Candes and Tao \cite{candes2007dantzig}. Candes and Tao \cite{candes2007dantzig} advocated an alternate method of estimating regressors called the Dantzig selector. The Dantzig selector estimates a sparse vector of coefficients, denoted $\hat{\underline{\beta}}^{\text{D}}$, by solving the objective function
\begin{equation}\label{eqn:dantzig_sel}
\underset{\underline{\beta}}{argmin}\ \vert \underline{\beta}\vert_1 + \lambda\vert X^T(\underline{y} -X\underline{\beta}) -k_p\sigma\vert_{\infty}.
 \end{equation}
Candes and Tao \cite{candes2007dantzig} suggested that this objective function be reformulated as a linear program. Linear programs have several highly reliable software applications to estimate the optimum. Candes and Tao \cite{candes2007dantzig} also gave a primal-dual interior point algorithm to solve the objective function with publicly available Matlab code. The authors emphasized a uniform uncertainty principle (UUP)\newabbrev{abbrev:UUP} and derived oracle and optimality results based on the UUP. Bickel, Ritov, and Tsybakov \cite{bickel2009simultaneous} and Koltchinksii \cite{koltchinskii2009dantzig} provided further theoretical analysis of the Dantzig selector, including optimality results and oracle inequalities under different conditions than Candes and Tao \cite{candes2007dantzig}.

 
 %%---------------------------------a paragraph on the Dirichlet process prior ?? --------------------------------%%%
  %Include??: Kim, S., Tadesse, M. G., & Vannucci, M. (2006). Variable selection in clustering via Dirichlet process mixture models. Biometrika, 93(4), 877-893.

 
 The last estimator we discuss is the horseshoe estimator, arising from the horseshoe prior. This prior was proposed by Gelman as a way to combat a numerical difficulty in MCMC sampling \cite{gelman2006prior}. Carvalho, Polson, and Scott \cite{carvalho2010horseshoe,carvalhohandling} describe the general setup. The horseshoe estimator arises via the hierarchical probability representation described in Equations \ref{eqn:horseshoe1}-\ref{eqn:horseshoe4} beneath 
 \begin{equation}\label{eqn:horseshoe1}
 \underline{y}\vert\underline{\beta} \sim N(X\underline{\beta}, \sigma^2I)
 \end{equation}  
 \begin{equation}\label{eqn:horseshoe2}
 \beta_j\vert\lambda_j,\tau \sim N(0, \lambda_j^2\tau^2)
 \end{equation}  
  \begin{equation}\label{eqn:horseshoe3}
  \pi(\lambda_j)\propto \frac{1}{1+\lambda_j^2}\mathds{1}[\lambda_j\geq0].
 \end{equation} 
   \begin{equation}\label{eqn:horseshoe4}
  \pi(\tau)\propto \frac{1}{1+\tau^2}\mathds{1}[\tau\geq0].
 \end{equation}   \newnot{symbol:sim} \newnot{symbol:normal} \newnot{symbol:propto}
 It is worth noting that the local scale priors $\lambda_j$ for $j=1, \dots, d$ are on the standard deviation scale and this prior is one of a general class of half-t densities \cite{gelman2006prior,polson2011half}. Carvalho et al. \cite{carvalhohandling} showed that the horseshoe prior has several appealing properties, including sparsity properties shared by the lasso estimator, while also having other desirable properties \emph{not} shared by the lasso. Carvalho et al. \cite{carvalho2010horseshoe,carvalhohandling} gave examples demonstrating the utility of this method for variable selection and indicated superior performance compared to the lasso in the datasets examined. Moreover, Polson \cite{polson2011half} argued that the half-t prior, or the half Cauchy prior as a special case, should become the reference (default) prior for variable selection problems. Polson justified this argument based on the many desirable properties of the horseshoe, some of which are not shared with other estimators such as the lasso. 
 
 % the following is a tentative \P that may or mat not be included, this paragraph attempts to link the variable selection 
 The attentive reader may notice that the majority of material in this subsection pertains to linear regression models. Decision tree models are inherently nonlinear in nature so it is reasonable to wonder: Will we use any of the material discussed in this section? The short answer is ``yes,'' but not in the way of extensions. Our approach will be more in terms of similarity of motivation and spirit. We explain the decision tree model we work with in the next chapter and show how we will exploit sparsity in said model by using modifications to distributions to encourage sparsity. Moreover, we will compare our proposed methods against many of the methods described in this section throughout this thesis. 
 
 We conclude this section by noting some review papers on the variable selection problem within the literature. O'Hara and Silanp\"{a}\"{a} \cite{o2009review} gave a recent notable Bayesian survey. This paper covered many of the methods discussed above in some detail and compared them all from a Bayesian perspective. Miller \cite{miller2002subset} gave a book length treatment on variable selection methods, with the second edition of the book including some Bayesian methods and the lasso, but not all methods discussed in this section. In particular, Miller \cite{miller2002subset} described the forward, backward and stepwise searches in detail and gave several useful references to the literature. From the CS literature, the review by Dash and Liu  \cite{dash1997feature} provided a useful reference into the CS field developments in variable selection. Dash and Liu \cite{dash1997feature} also provided a useful framework to compare subset/variable selection approaches. George \cite{george2000variable} provided a good overview and major references in the variable or subset selection field current to the year 2000.     
 
 \newpage
 
 