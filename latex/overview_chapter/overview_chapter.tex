\section{Preliminaries}
\label{ch:preliminaries}
This chapter provides an overview of the models used for decision tree induction and variable selection. We begin by discussing the earliest methods of induction: greedy algorithms. With the advance of time (about a decade) greedy induction approaches became fully developed high quality fortran codes. Bayesian approaches to building decision trees were not possible until advances in computing power and the rediscovery of sampling based approaches to Bayesian statistics became popular again. While the work of Breiman et al \cite{breiman1984classification} always contained a Bayesian flavor, no probability measure over trees was ever proposed. That is until the groundbreaking works of Denison, Mallick and Smith \cite{denison1998bayesian} and Chipman, George, and Mcculloch \cite{chipman1998bayesian}.

 During the same time (the 1990s and the 2000s), researchers in Machine Learning and Statistics were developing theory beyond that published by Breiman et al. \cite{breiman1984classification}, that ensured decision trees were consistent. In this case consistency means the error rate of the tree converges to the Bayes error. The initial impetus for this work stemmed from the landmark paper of Vapnik and Chervonenkis \cite{vapnik1971uniform}, and subsequent work \cite{vapnik2000nature}, though it would take over a decade for the full power of their combinatorial approach to be widely used in both Machine Learning and Statistics. 

This chapter collects necessary preliminaries for the reader to understand the developments in subsequent chapters. Therefore the reader may omit this chapter on a first reading and refer back to the necessary subsections when subsequent chapters recall the preliminaries. Subsection \ref{ch:p1sub_greedy_ind} and subsection \ref{ch:p2sub_bayes} review the necessary material on greedy and Bayesian approaches to decision tree induction. The reader who is not familiar with these two approaches to decision trees should read these two subsections before moving on to the rest of the thesis.  

\subsection{Greedy Induction}
\label{ch:p1sub_greedy_ind}

In this section we answer the question: how does greedy induction work? Once the reader has completed this section they should, with sufficient time, be able to reproduce computer codes that would induct decision trees using greedy strategies. We begin with impurity functions. 

\subsubsection{Impurity Functions}

An impurity function corresponds to a likelihood function in statistics or to an objective function in machine learning. This function measures in some sense, how many good observations lie in a node of the tree and how many bad observations lie in a node of the tree. Formally an impurity function, denoted $\phi$ must satisfy three axioms. 

\begin{enumerate}
\item $\phi(1/2,1/2) \geq \phi(p, 1-p)$ for any $p\in[0,1]$. 
\item $\phi(0,1)=\phi(1,0)=0$. 
\item $ \phi(p, 1-p)$ non-decreasing for $p\in[0,1/2]$ and non-increasing for $p\in[1/2,1]$. 
\end{enumerate}

The impurity functions given in Breiman et al. \cite{breiman1984classification} are: 
\begin{enumerate}
\item Entropy: $\phi(p,1-p)= -plog(p) -(1-p)log(1-p)$.
\item Gini: $\phi(p, 1-p)=2p(1-p)$.
\item Misclass probability: $\phi(p, 1-p)=min(p,1-p).$
\end{enumerate}
If a regression tree is to be built/inducted, in place of an impurity function, we use the mean squared error criteria denoted here as  

$$\text{MSE}: \phi(y_i, \bar{y})=\sum_i(y_i-\bar{y})^2.$$
The greedy approach to decision tree induction always tries to optimize impurity functions. The optimization problems are known to be NP-Hard (for an introduction to complexity theory c.f. Garey and Johnson \cite{garey1979computers}). Briefly NP-Hard optimization problems are considered some of the hardest optimization problems to solve. These optimization problems increase exponentially in the size of the problem. Therefore, exact methods would nearly always take too long to compute and thus greedy strategies are used to approximate the global optimum, assuming one exists, with a local optimum. 
\subsubsection{Induction}
The induction of decision trees proceeds by solving the objective function

\begin{equation}
\underset{t,s}{argmax} \Delta\phi = \phi - \pi_L\phi_L-\pi_R\phi_R.
\end{equation}
Here the variable $t$ and $s$ scroll over all covariates in the data and all midpoints between successive observations (or at observed points) of the $t$th covariate respectively. The proportions $\pi_L$ and $\pi_R$ represent the number of data points going into the node to the left ($L$) and right ($R$) of the current node if the chosen split is on covariate $t$ and observation $s$, and similarly defined subscripted impurity functions. Thus, if there are 100 observations in the current node and as a result of the split on covariate $t$, at observation $s$, 70 observations go into the left child node and 30 observations go into the right child node, then the two proportions are $(\pi_L, \pi_R) = (0.7, 0.3)$. 

The tree induction process continues until no more data points are incorrectly classified, or a predetermined stopping rule is met. Once the full tree has been built, the second stage of the process now starts. This is known as the pruning stage. Now that the full tree is grown, we progressively prune back terminal nodes of the tree until the root node occurs. Several related approaches have been proposed in the literature to select the optimal tree via pruning. The most common is to select the tree using the regularized risk estimate given in Equation \ref{eqn:reg_risk_est} \newnot{symbol:risk_tree}

\begin{equation}\label{eqn:reg_risk_est}
R(\mathcal{T}_i,\alpha) = R(\mathcal{T}_i) + \alpha \vert \mathcal{T}_i \vert. 
\end{equation}

Here the $\alpha$ parameter is a regularization parameter with larger values of $\alpha$ given greater penalty to the number of terminal nodes in the tree, here denoted $\vert \mathcal{T}_i\vert$. The notation $R(\mathcal{T}_i)$ denotes the risk of the tree, which is usually calculated as the sum of squared errors across all terminal nodes in a regression setting or the sum of the impurity function values in each terminal of the tree in the classification case. The choice of $\alpha$ is done over a grid of positive values on holdout data, using a cross validation approach, and the value of $\alpha$ leading to the smallest regularized risk ($R(\mathcal{T}_i,\alpha)$) is chosen.  

Discussion of consistency of this pruning rule can be found in Devroye et al. \cite{devroye1996probabilistic}, Breiman et al. \cite{breiman1984classification}, Gey \cite{gey2005model}, and Suav\'{e} and Tuleau-Malot \cite{sauve2011variable}. All the theoretical results require controlling the complexity of the decision tree, $\vert\mathcal{T}\vert$, and allowing the number of data points $n\to\infty$. However, the results in Devroye et al. \cite{devroye1996probabilistic} also give explicit bounds on the error of decision tree classifiers for finite values of $n$. 

The pruning rule discussed here, and the induction process overall, is an implicit form of model selection. This is implicit because the variables that are selected are considered important and those variables not selected are considered not important. Furthermore, no measure of importance on each variable is defined, so it is difficult to rank variables based on importance. A remedy, called variable importance (abbreviated VIMP) \newabbrev{abbrev:VIMP}was proposed in the literature by Breiman \cite{breiman2001random}, but this is in the context of random forests and not for a single decision tree. We proposed several different methods to perform explicit variable selection for Bayesian decision trees in later chapters.   

\subsubsection{A Simple Example}

In this subsection we work through a simple example to give the reader a flavor of the calculations necessary to induct a decision tree. 

Consider the following data 

\begin{table}[H]
\begin{center}
\begin{tabular}{ l | c | c | c }
  i& $y_i$ & $x_1$ & $x_2$\\
  \hline
  1&1 & 2 & 3 \\
  2&2 & 5 & 6 \\
  3&5 & 8 & 9 \\
\end{tabular}
\caption[A simple decision tree example data]{A simple decision tree example data.}
\label{fig:dtree_firststep}
\end{center}
\end{table}

We have 3 observations and two covariates within each observation. The response is a continuous random variable, so this will be a regression tree approach. We will examine potential split points by looking at the midpoints between two observed values of the covariates. We begin by sorting the data in increasing order for both covariates. Fortunately, in this case, the data is already in such a sorted order for both covariates, so no sorting is necessary. We now examine the possibility of a split point on $x_1$. Using the MSE \newabbrev{abbrev:MSE} impurity we calculate $\phi$, $\phi_L$, and $\phi_R$. A sum of squared error calculation shows $\phi = 26/3$ which is a constant for all calculations we perform. 

Now for a split between observation 1 and 2:
$$
\Delta\phi=26/3- (1/3)(1-1)^2 - (2/3)((2-7/2)^2+(5-7/2)^2)=26/3-9/3=17/3
$$ 

and for a split between observation 2 and 3: 
$$
\Delta\phi=26/3 - (2/3)( (1-3/2)^2 + (2-3/2)^2)  - (5-5)^2(1/3)= 26/3-1/3=25/3 
$$ 
Now, because all the data is sorted, the same $\Delta\phi$ values will result for potential splits on $x_2$. Therefore, we (somewhat arbitrarily) choose to split on the covariate with the smaller index ($x_1$). Thus, our first split is on the value $\{x_1: x_1 \leq 6.5\}$ and the tree at this point looks like that in Figure \ref{fig:dtree_firststep}. Note the mean of the values in each terminal node is given in Figure \ref{fig:dtree_firststep}. The mean value would be the predicted value for observations falling into the given terminal node.

\begin{figure}
\begin{center}
\tikzset{every tree node/.style={minimum width=2em,draw,circle},
         blank/.style={draw=none},
         edge from parent/.style=
         {draw, edge from parent path={(\tikzparentnode) -- (\tikzchildnode)}},
         level distance=1.5cm}
\begin{tikzpicture}
\Tree
[. $x_1\leq 6.5$     
    [.7/2 ]
    [.5  
    \edge[blank]; \node[blank]{};
            ]
    ]
]
\end{tikzpicture}
\end{center}
\caption[A simple decision tree]{The decision tree after the first split.}
\label{fig:dtree_firststep}
\end{figure}
We would then continue calculating $\Delta\phi$s for the resulting data that falls into each of the two resultant terminal nodes, continuing until there is only one observation in each terminal node, or until there is some specified number of observations in each terminal node. Although any number is possible, the specified minimum number of observations in each terminal node is usually 5. Once this process is completed, the induction step is finished, and the pruning process begins. 

%\subsection{Bayesian Approaches}
%\label{ch:p2sub_bayes}
% the section and label are contained in the file with filepath beneath
\input{overview_chapter/bayesian_approaches_section/bayes_decision_tree.tex}

\subsection{Previous Variable Selection}
\label{ch:p4sub_var_sel}

Previous approaches to variable selection have focused primarily on the linear model or GLM or GLMM models, all of which we briefly reviewed in Chapter \ref{ch:intro}. Hereafter we will refer to all three types of models as linear. The correspondence between variable selection in linear models and in Bayesian decision trees is a simple correspondence between zeros in the linear model, or zero means of the normal and transformed values on the unit simplex. This correspondence will be detailed in the next subsection of this chapter.   

Ishwaran et al. \cite{ishwaran2010high} propose a modification to Variable important (VIMP) criteria that allows some very basic theory to describe the VIMP's analytical properties. VIMP was first proposed by Breiman \cite{breiman2001random} as a method to assess which covariates are important in a dataset. The difficulty with both Ishwaran et al's method and Breiman's method is that they are both very much black box techniques. VIMP basically takes a split in a decision tree on a specific covariate and randomly decides if the observation should go to the left or the right child node. This is done for a collection of observations and the random predictions are averaged against the actual predictions. The resulting numeric estimates for each covariate are the VIMP estimates. A ranking of these values provides a ranking of the covariates. 

A similar method of ranking covariates was proposed by Taddy, Gramacy and Polson \cite{taddy2011dynamic}. Taddy, Gramacy and Polson proposed a Bayesian approach and used the Bayesian equivalent to $\Delta\phi$ from this chapter. In the Bayesian approach the quantity $\Delta\phi$ really has little meaning, because we are using an MCMC approach to search across trees. The resulting estimates of the covariate's importance will be basically the same as the greedy approach. Something we find undesirable for many reasons. These methods are worth comparing against our approach and we will do so in future work.
 
\begin{equation}
\Delta\phi = \int\phi ds - \pi_L\int\phi ds -\pi_R \int \phi_Rds. 
 \end{equation}
Taddy, Gramacy and Polson propose using samples and estimating the integrals with Monte Carlo approximations. In this case the approach is using a greedy measure on a Bayesian problem, something we find confusing.
 
%\textbf{Write in a brief description of both Ishwaran et al approach and Gramacy and Lee approach}. 

%\input{overview_chapter/previous_variable_selection_section/previous_var_selection.tex}

\subsection{Derivations}
\label{ch:p3sub_theory}

This subsection contains the mathematical details for calculating the integrals to get the closed form solutions for the integrated likelihood equations in this chapter.   

%\subsubsection{Normal Derivation}

\subsubsection{ZIP Derivations}

In this section we provide the derivation for the integrated likelihood for the Bayes ZIP\newabbrev{abbrev:ZIP} (zero inflated poisson) tree model. Now let us define some notation: $j$ will index either all observations or only the observed zero count observations if the upper limit is $n_0$ then $j$ will index observed zero counts only, if the upper index limit is $n$ then all observations are indexed. Also $j^{\prime}$ will index the non-zero observations. The total number of non-zero observations is denoted $n_+$, so that $n_++n_0=n$. Finally let $\bar{y}_{i+}$ denote the sample mean of the non-zero count observations in terminal node $i$. 

\begin{align*}
\Pr(Y \vert X, \mathcal{T}) &= \prod_{i=1}^b\int_0^1\int_0^\infty\prod_{j=1}^{n_i}\left[\mathds{1}[y_{ij}=0](\phi+(1-\phi)\exp{(-\lambda)})+\mathds{1}[y_{ij}>0]\frac{\exp{(-\lambda)\lambda^{y_{ij}}}}{y_{ij}!} \right]\pi(\phi_i,\lambda_i )d\lambda_id\phi_i\\
&=\prod_{i=1}^b \int_0^1\int_0^\infty\underbrace{\prod_{j=1}^{n_0}(\phi + (1-\phi)\exp{(-\lambda)})\pi(\phi_i,\lambda_i )d\lambda_id\phi_i}_{=(1)}\\ 
&+ \prod_{i=1}^b \int_0^1\int_0^\infty \underbrace{\prod_{j^\prime=1}^{n_+}\frac{\exp{(-\lambda)}\lambda^{y_{ij^\prime}} }{y_{ij^\prime}!}\pi(\phi_i,\lambda_i )d\lambda_id\phi_i}_{=(2)}.\\ 
\end{align*} 

We will first tackle $(1)$, then tackle $(2)$. 

\begin{align*}
(1)&= \int_0^1\int_0^\infty\prod_{j=1}^{n_0}(\phi + (1-\phi)\exp{(-\lambda)})\pi(\phi_i,\lambda_i )d\lambda_id\phi_i \\
&= \int_0^1\int_0^\infty(\phi + (1-\phi)\exp{(-\lambda)})^{n_0}\pi(\phi_i,\lambda_i )d\lambda_id\phi_i \\
&= \int_0^1\int_0^\infty\sum_{j=1}^{n_0}{n_0\choose j}\phi^{j}(1-\phi)^{n_0-j}\exp{(-(n_0-j)\lambda)})\pi(\phi_i)\pi(\lambda_i )d\lambda_id\phi_i. \\
\end{align*}
 
 Now we take $\pi(\phi_i)$ to be a beta($\alpha, \beta$) prior and $\pi(\lambda_i)$ to be a gamma($\alpha_{\lambda}, \beta_{\lambda}$) prior. This simplifies matters greatly. 
 
 \begin{align*}
 & \int_0^1\int_0^\infty\sum_{j=1}^{n_0}{n_0\choose j}\phi^{j}(1-\phi)^{n_0-j}\exp{(-(n_0-j)\lambda)})\frac{\Gamma(\alpha+\beta)\phi^{\alpha-1}(1-\phi)^{\beta-1}}{\Gamma(\alpha)\Gamma(\beta)}\frac{\lambda^{\alpha_{\lambda}-1}\exp{(-\lambda/\beta_{\lambda})}}{\Gamma(\alpha_{\lambda})\beta_{\lambda}^{\alpha_{\lambda}}} d\lambda_id\phi_i \\
&=\sum_{j=1}^{n_0}{n_0\choose j}\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)\Gamma(\alpha_{\lambda})\beta_{\lambda}^{\alpha_{\lambda}}} \underbrace{\int_0^1\phi^{j+\alpha-1}(1-\phi)^{\beta+n_0-j-1}d\phi_i}_{\text{a beta kernel}}  \underbrace{\int_0^\infty \lambda^{\alpha_{\lambda}-1} \exp{(-(n_0-j+\beta_{\lambda}^{-1})\lambda)} d\lambda_i}_{\text{a gamma kernel}}\\
&=\underbrace{\sum_{j=1}^{n_0}{n_0\choose j}\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)\Gamma(\alpha_{\lambda})\beta_{\lambda}^{\alpha_{\lambda}}} \frac{\Gamma(\alpha+j)\Gamma(\beta+n_0-j)\Gamma(\alpha_{\lambda})}{\Gamma(\alpha+\beta+n_0)}(n_0-j+\beta_{\lambda}^{-1})^{\alpha_{\lambda}}}_{=(1)}.
\end{align*} 
Now with the first piece simplified we move on to piece $(2)$. 
 
 \begin{align*}
 (2) &= \int_0^1\int_0^\infty\prod_{j^\prime=1}^{n_+}\frac{\exp{(-\lambda)}\lambda^{y_{ij^\prime}} }{y_{ij^\prime}!}\pi(\phi_i,\lambda_i )d\lambda_id\phi_i \\
 &= \int_0^\infty \prod_{j^\prime=1}^{n_+}\frac{\exp{(-\lambda)}\lambda^{y_{ij^\prime}} }{y_{ij^\prime}!}\pi(\lambda_i )d\lambda_i \\
 &= \int_0^\infty\frac{\exp{(-n_+\lambda)}\lambda^{n_+\bar{y}_{i+}} }{\prod_{j^\prime=1}^{n_+}y_{ij^\prime}!}\pi(\lambda_i )d\lambda_i \\
 &= \int_0^\infty\frac{\exp{(-n_+\lambda)}\lambda^{n_+\bar{y}_{i+}} }{\prod_{j^\prime=1}^{n_+}y_{ij^\prime}!}\frac{\lambda^{\alpha_{\lambda}-1}\exp{(-\lambda/\beta_{\lambda})}}{\Gamma(\alpha_{\lambda})}d\lambda_i \\
 &= \frac{\int_0^\infty\exp{(-(n_+ +\beta_{\lambda}^{-1})\lambda)}\lambda^{n_+\bar{y}_{i+} +\alpha_{\lambda}-1} d\lambda_i}{\Gamma(\alpha_{\lambda})\prod_{j^\prime=1}^{n_+}y_{ij^\prime}!} \\
 &= \underbrace{\frac{\Gamma(n_+\bar{y}_{i+}+\alpha_{\lambda})(n_+ +\beta_{\lambda}^{-1})^{n_+\bar{y}_{i+}+\alpha_{\lambda}}}{\Gamma(\alpha_{\lambda})\prod_{j^\prime=1}^{n_+}y_{ij^\prime}!}}_{=(2)}. \\
 \end{align*}
 
 And the result is shown. 

%%%%%---------------------- Miscellaneous Notes --------------------------------------


% Np-hardness of decision tree induction-> Some Rivest 1980's papers (i think there are two different ones.)

% need to define P, NP, NP hard, NP-complete any others? P#? 

%consistency of decision trees. Define consistency, 


% want to include the works of Suave and Tuleau-Mallot, Gey and Nedelic, Breiman, Devroye, Gyorfi, Lugosi, Biau and Devroye, Biau 
